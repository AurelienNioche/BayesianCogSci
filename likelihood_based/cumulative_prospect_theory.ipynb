{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative Prospect Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Part of this is an adaption of https://towardsdatascience.com/an-introduction-to-bayesian-inference-in-pystan-c27078e58d53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related documentation:\n",
    "* https://mc-stan.org/docs/2_23/stan-users-guide/example-decision-analysis.html\n",
    "* https://mc-stan.org/docs/2_25/stan-users-guide/examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pystan\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from scipy.special import expit\n",
    "multiprocessing.set_start_method(\"fork\")\n",
    "sns.set()  # Nice plot aesthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility of an outcome\n",
    "\n",
    "The utility of a normalized outcome $x \\in [0, 1]$ is defined as:\n",
    "\\begin{equation}\n",
    "u(x) = x^{1 - \\alpha}\n",
    "\\end{equation}\n",
    "with\n",
    "$\\alpha \\in [-\\infty, 1]$\n",
    "a free parameter describing the risk-aversion of the decision-maker (Holt & Laury, 2002). If $\\alpha$\n",
    "is positive,\n",
    "$u''$\n",
    "is negative, indicating risk-averse preferences (VNM, 1944). If\n",
    "$\\alpha$\n",
    "is negative,\n",
    "$u''$ \n",
    "is positive, indicating risk-seeking preferences. If\n",
    "$\\alpha$\n",
    "is equal to 0,\n",
    "$\\forall x: u(x) = x$ and $u''(x) = 0$,\n",
    "indicating risk-neutral preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjective probability perception \n",
    "$w$, the subjective probability perception function is defined following the formulation of Prelec (1998) as:\n",
    "\\begin{equation}\n",
    "w(p) = e^{ - ( - \\ln p )^{\\beta}} \n",
    "\\end{equation}\n",
    "with $p\\in (0, 1]$\n",
    "the actual probability, and with $\\beta \\in (0, \\infty)$,\n",
    "a free parameter indicating the distortion of the probability\n",
    "perception. We assume that $w(0) = 0$. For $\\beta \\in (0, 1)$,\n",
    "the closer\n",
    "$\\beta$\n",
    "is to zero, the more the small probabilities are overestimated, and the\n",
    "large probabilities underestimated. For\n",
    "$\\beta=1$,\n",
    "the subjective probabilities are the same as the objective\n",
    "probabilities. For\n",
    "$\\beta \\in (1, \\infty)$,\n",
    "the higher\n",
    "$\\beta$,\n",
    "the more the small probabilities are underestimated, and the large\n",
    "probabilities overestimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjective expected utility\n",
    "\n",
    "The subjective expected utility of a lottery $L = \\{(p_1, x_1), \\dots, (p_n, x_n)\\}$ is defined as:\n",
    "\\begin{equation}\n",
    "SEU(L) = \\sum_{i=1}^{n} \\pi_i u(x_i)\n",
    "\\end{equation}\n",
    "where $\\pi_i$(for gains) is:\n",
    "\\begin{equation}\n",
    "\\pi_i = w\\left(\\sum_{k=i}^{n} p_k \\right) - w\\left(\\sum_{k=i+1}^{n} p_k\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability to choose a lottery\n",
    "\n",
    "The probability to choose the lottery $L_k \\in \\mathcal{L} = \\{L_1, \\dots, L_{N}\\}$ is defined as: \n",
    "\\begin{equation}\n",
    "P(L_k) = \\frac {e^{SEU(L_k) / \\tau}}{\\sum _{i=1}^{N} e^{SEU(L_i) / \\tau } }\n",
    "\\end{equation}\n",
    "$\\tau \\in (0, \\infty)$\n",
    "a free parameter describing to which extent decision-making is stochastic.\n",
    "The higher\n",
    "$\\tau$,\n",
    "the more the decision-making is stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We consider here a special case where $L = \\{(p, x)\\}$ (one lottery has one possible positive output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stimuli(n_trial, n_lot):\n",
    "    \n",
    "    possible_p = [0.25, 0.5, 0.75, 1]\n",
    "    possible_x = np.arange(1, 4)\n",
    "    p = np.random.random(size=(n_trial, 2))\n",
    "    x = np.array([np.random.choice(possible_x, size=2, replace=False) for _ in range(n_trial)])\n",
    "    x = -np.sort(-x, axis=-1)\n",
    "    return pd.DataFrame({\"p0\": p[:, 0], \"p1\": p[:, 1], \"x0\": x[:, 0], \"x1\": x[:, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(param, n_trial):\n",
    "\n",
    "    data = generate_stimuli(n_trial=n_trial)\n",
    "    \n",
    "    alpha, tau, beta = param\n",
    "\n",
    "    n = len(data)\n",
    "\n",
    "    sp0 = np.exp(-(-np.log(data.p0.values)) ** beta)\n",
    "    sp1 = np.exp(-(-np.log(data.p1.values)) ** beta)\n",
    "    \n",
    "    su0 = data.x0.values ** (1 - alpha)\n",
    "    su1 = data.x1.values ** (1 - alpha)\n",
    "    \n",
    "    v0 = sp0 * su0\n",
    "    v1 = sp1 * su1\n",
    "    \n",
    "    delta = v0 - v1\n",
    "    \n",
    "    p = np.zeros((n, 2))\n",
    "    p[:, 0] = expit(delta/tau)\n",
    "    p[:, 1] = 1 - p[:, 0]\n",
    "    \n",
    "    c = np.zeros(n, dtype=int)\n",
    "    r = np.random.random(size=n)\n",
    "    c[:] = p[:, 1] > r\n",
    "    \n",
    "    data[\"c\"] = c\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       p0   p1  x0  x1  c\n",
       "0    0.25  1.0   3   2  1\n",
       "1    0.50  1.0   2   1  0\n",
       "2    0.25  1.0   3   1  1\n",
       "3    0.25  1.0   3   2  1\n",
       "4    0.75  1.0   3   2  1\n",
       "..    ...  ...  ..  .. ..\n",
       "995  0.25  0.5   3   2  1\n",
       "996  0.25  1.0   3   1  1\n",
       "997  0.25  0.5   3   1  0\n",
       "998  0.50  1.0   2   1  1\n",
       "999  0.75  1.0   3   2  1\n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True parameters\n",
    "alpha_true = 0.3\n",
    "tau_true = 0.1\n",
    "beta_true = 0.8\n",
    "\n",
    "# Number of trials\n",
    "n_trial = 1000\n",
    "\n",
    "# Generate parameters to simualte\n",
    "param_true = alpha_true, tau_true, beta_true\n",
    "\n",
    "# Simulate\n",
    "d = simulate(param=param_true, n_trial=n_trial)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.549577</td>\n",
       "      <td>0.242381</td>\n",
       "      <td>0.674709</td>\n",
       "      <td>0.152820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.613223</td>\n",
       "      <td>0.458828</td>\n",
       "      <td>0.735215</td>\n",
       "      <td>0.431809</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.841033</td>\n",
       "      <td>0.514185</td>\n",
       "      <td>0.962716</td>\n",
       "      <td>0.357361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.825513</td>\n",
       "      <td>0.167835</td>\n",
       "      <td>0.216763</td>\n",
       "      <td>0.983442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.782138</td>\n",
       "      <td>0.239888</td>\n",
       "      <td>0.514050</td>\n",
       "      <td>0.563739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.569042</td>\n",
       "      <td>0.728209</td>\n",
       "      <td>0.379095</td>\n",
       "      <td>0.024971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.788716</td>\n",
       "      <td>0.342232</td>\n",
       "      <td>0.026529</td>\n",
       "      <td>0.690979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.250664</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.372233</td>\n",
       "      <td>0.273774</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.348286</td>\n",
       "      <td>0.018089</td>\n",
       "      <td>0.565436</td>\n",
       "      <td>0.883045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.555725</td>\n",
       "      <td>0.171067</td>\n",
       "      <td>0.333025</td>\n",
       "      <td>0.454004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        N        x0        x1        p0        p1  y\n",
       "0    1000  0.549577  0.242381  0.674709  0.152820  0\n",
       "1    1000  0.613223  0.458828  0.735215  0.431809  0\n",
       "2    1000  0.841033  0.514185  0.962716  0.357361  0\n",
       "3    1000  0.825513  0.167835  0.216763  0.983442  0\n",
       "4    1000  0.782138  0.239888  0.514050  0.563739  0\n",
       "..    ...       ...       ...       ...       ... ..\n",
       "995  1000  0.569042  0.728209  0.379095  0.024971  0\n",
       "996  1000  0.788716  0.342232  0.026529  0.690979  1\n",
       "997  1000  0.250664  0.000489  0.372233  0.273774  1\n",
       "998  1000  0.348286  0.018089  0.565436  0.883045  1\n",
       "999  1000  0.555725  0.171067  0.333025  0.454004  0\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True parameters\n",
    "alpha_true = 0.3\n",
    "tau_true = 0.1\n",
    "\n",
    "# Number of trials\n",
    "n_trial = 1000\n",
    "\n",
    "# Number of lottery per trial\n",
    "n_lot = 2\n",
    "\n",
    "p = np.random.random(size=(n_lot, n_trial))\n",
    "x = np.random.random(size=(n_lot, n_trial))\n",
    "\n",
    "u = x**(1-alpha_true)\n",
    "eu = p * u\n",
    "\n",
    "p_choice = np.exp(eu/tau_true)\n",
    "p_choice /= p_choice.sum(axis=0)\n",
    "\n",
    "y = np.zeros(n_trial, dtype=int)\n",
    "y[:] = p_choice[1] > np.random.random(size=n_trial)\n",
    "\n",
    "data = {'N': n_trial, 'x0': x[0], 'x1': x[1], 'p0': p[0], 'p1': p[1], 'y': y}\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using Stan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in Stan language, vector indexes start from 1 (unlike Python thats starts from 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"\"\"\n",
    "functions {\n",
    "    vector w(vector p, real gam){\n",
    "        int p_len = dims(p)[1];\n",
    "        vector[p_len] w;\n",
    "        for(i in 1:p_len){\n",
    "            real p_i = p[i];\n",
    "            //Yang 2017 w-function eq(5)\n",
    "            // ret[ii] = p_i^gam / ( p_i^gam + (1-p_i)^gam )^(1 / gam);\n",
    "            w[ii] = p_i^gam / ( p_i^gam + (1-p_i)^gam )^(1 / gam);\n",
    "    }\n",
    "    return w;\n",
    "  }\n",
    "  real w_real(real p, real gam){\n",
    "    real ret = p^gam / ( p^gam + (1-p)^gam )^(1 / gam);\n",
    "    return(ret);\n",
    "  }\n",
    "  vector prob_distort(vector p, real gam){\n",
    "    int p_len = dims(p)[1];\n",
    "    vector[p_len] pi_p;\n",
    "    for(kk in 1:(p_len-1)){\n",
    "      pi_p[kk] = w_real(sum(p[kk:p_len]), gam) - w_real(sum(p[(kk+1):p_len]), gam);\n",
    "    }\n",
    "    pi_p[p_len] = 1 - sum(pi_p[1:(p_len-1)]);\n",
    "    return(pi_p);\n",
    "  }\n",
    "  vector value_func(vector x, real alpha, real lambda, real eta){\n",
    "    int N_val = dims(x)[1];\n",
    "    vector[N_val] ret;\n",
    "    for(ii in 1:N_val){\n",
    "      real xi = x[ii];\n",
    "      if(xi < 0){\n",
    "        ret[ii] = - lambda * (-xi)^eta;\n",
    "      }else{\n",
    "        ret[ii] = xi^alpha;\n",
    "      }\n",
    "    }\n",
    "    return(ret);\n",
    "  }\n",
    "}\n",
    "data {\n",
    "  int<lower = 1> TT; //Number of choices\n",
    "  int<lower = 1> N_lotto;\n",
    "  int<lower = 1> N_choices_max; //Max number of choices\n",
    "  int N_choices[N_lotto];\n",
    "  int<lower = 1, upper = N_lotto> choices[TT]; //the actual choices\n",
    "  matrix[N_lotto, N_choices_max] lotto_probabilities;\n",
    "  matrix[N_lotto, N_choices_max] lotto_rewards;\n",
    "}\n",
    "parameters {\n",
    "  real<lower=0, upper=1> alpha;\n",
    "  real<lower=0, upper=1> eta;\n",
    "  real<lower=1, upper=2> lambda;\n",
    "  real<lower=0, upper=1> gamma;\n",
    "}\n",
    "transformed parameters{\n",
    "  simplex[N_lotto] p;\n",
    "  vector[N_lotto] CPV;\n",
    "  \n",
    "  for(ii in 1:N_lotto){\n",
    "    int n_choices_lotto_i = N_choices[ii];\n",
    "    vector[n_choices_lotto_i] probs_i = to_vector(lotto_probabilities[ii, 1:n_choices_lotto_i]);\n",
    "    vector[n_choices_lotto_i] rewards_i = to_vector(lotto_rewards[ii, 1:n_choices_lotto_i]);\n",
    "    CPV[ii] =  sum(prob_distort( probs_i, gamma) .* value_func( rewards_i, alpha, lambda, eta)); //subjective expectation\n",
    "  }\n",
    "  p = softmax(CPV);\n",
    "}\n",
    "model {\n",
    "  alpha ~ beta(1,1);\n",
    "  eta ~ beta(1,1);\n",
    "  lambda ~ uniform(1,2);\n",
    "  gamma ~ beta(1,1);\n",
    "  for(ii in 1:TT){\n",
    "    target += log(p[choices[ii]]);\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"\"\"\n",
    "functions {\n",
    "    vector w(vector p, real gam){\n",
    "        int p_len = dims(p)[1];\n",
    "        vector[p_len] w;\n",
    "        for(i in 1:p_len){\n",
    "            real p_i = p[i];\n",
    "            //Yang 2017 w-function eq(5)\n",
    "            // ret[ii] = p_i^gam / ( p_i^gam + (1-p_i)^gam )^(1 / gam);\n",
    "            w[ii] = p_i^gam / ( p_i^gam + (1-p_i)^gam )^(1 / gam);\n",
    "    }\n",
    "    return w;\n",
    "  }\n",
    "  real w_real(real p, real gam){\n",
    "    real ret = p^gam / ( p^gam + (1-p)^gam )^(1 / gam);\n",
    "    return(ret);\n",
    "  }\n",
    "  vector prob_distort(vector p, real gam){\n",
    "    int p_len = dims(p)[1];\n",
    "    vector[p_len] pi_p;\n",
    "    for(kk in 1:(p_len-1)){\n",
    "      pi_p[kk] = w_real(sum(p[kk:p_len]), gam) - w_real(sum(p[(kk+1):p_len]), gam);\n",
    "    }\n",
    "    pi_p[p_len] = 1 - sum(pi_p[1:(p_len-1)]);\n",
    "    return(pi_p);\n",
    "  }\n",
    "  vector value_func(vector x, real alpha, real lambda, real eta){\n",
    "    int N_val = dims(x)[1];\n",
    "    vector[N_val] ret;\n",
    "    for(ii in 1:N_val){\n",
    "      real xi = x[ii];\n",
    "      if(xi < 0){\n",
    "        ret[ii] = - lambda * (-xi)^eta;\n",
    "      }else{\n",
    "        ret[ii] = xi^alpha;\n",
    "      }\n",
    "    }\n",
    "    return(ret);\n",
    "  }\n",
    "}\n",
    "data {\n",
    "  int<lower = 1> TT; //Number of choices\n",
    "  int<lower = 1> N_lotto;\n",
    "  int<lower = 1> N_choices_max; //Max number of choices\n",
    "  int N_choices[N_lotto];\n",
    "  int<lower = 1, upper = N_lotto> choices[TT]; //the actual choices\n",
    "  matrix[N_lotto, N_choices_max] lotto_probabilities;\n",
    "  matrix[N_lotto, N_choices_max] lotto_rewards;\n",
    "}\n",
    "parameters {\n",
    "  real<lower=0, upper=1> alpha;\n",
    "  real<lower=0, upper=1> eta;\n",
    "  real<lower=1, upper=2> lambda;\n",
    "  real<lower=0, upper=1> gamma;\n",
    "}\n",
    "transformed parameters{\n",
    "  simplex[N_lotto] p;\n",
    "  vector[N_lotto] CPV;\n",
    "  \n",
    "  for(ii in 1:N_lotto){\n",
    "    int n_choices_lotto_i = N_choices[ii];\n",
    "    vector[n_choices_lotto_i] probs_i = to_vector(lotto_probabilities[ii, 1:n_choices_lotto_i]);\n",
    "    vector[n_choices_lotto_i] rewards_i = to_vector(lotto_rewards[ii, 1:n_choices_lotto_i]);\n",
    "    CPV[ii] =  sum(prob_distort( probs_i, gamma) .* value_func( rewards_i, alpha, lambda, eta)); //subjective expectation\n",
    "  }\n",
    "  p = softmax(CPV);\n",
    "}\n",
    "model {\n",
    "  alpha ~ beta(1,1);\n",
    "  eta ~ beta(1,1);\n",
    "  lambda ~ uniform(1,2);\n",
    "  gamma ~ beta(1,1);\n",
    "  for(ii in 1:TT){\n",
    "    target += log(p[choices[ii]]);\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to parse Stan model 'anon_model_c0c90454372f58626ca4a5c420cb1f69'. Error message:\nSYNTAX ERROR, MESSAGE(S) FROM PARSER:\nVariable \"ii\" does not exist.\n error in 'unknown file name' at line 10, column 16\n  -------------------------------------------------\n     8:             //Yang 2017 w-function eq(5)\n     9:             // ret[ii] = p_i^gam / ( p_i^gam + (1-p_i)^gam )^(1 / gam);\n    10:             w[ii] = p_i^gam / ( p_i^gam + (1-p_i)^gam )^(1 / gam);\n                       ^\n    11:     }\n  -------------------------------------------------\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4361b22a3ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpystan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStanModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pystan/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, charset, model_name, model_code, stanc_ret, include_paths, boost_lib, eigen_lib, verbose, obfuscate_model_name, extra_compile_args, allow_undefined, include_dirs, includes)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstanc_ret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             stanc_ret = pystan.api.stanc(file=file,\n\u001b[0m\u001b[1;32m    231\u001b[0m                                          \u001b[0mcharset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                                          \u001b[0mmodel_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pystan/api.py\u001b[0m in \u001b[0;36mstanc\u001b[0;34m(file, charset, model_code, model_name, include_paths, verbose, obfuscate_model_name, allow_undefined)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Failed to parse Stan model '{}'. Error message:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# SUCCESS_RC is 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Successfully parsed Stan model '{}'.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to parse Stan model 'anon_model_c0c90454372f58626ca4a5c420cb1f69'. Error message:\nSYNTAX ERROR, MESSAGE(S) FROM PARSER:\nVariable \"ii\" does not exist.\n error in 'unknown file name' at line 10, column 16\n  -------------------------------------------------\n     8:             //Yang 2017 w-function eq(5)\n     9:             // ret[ii] = p_i^gam / ( p_i^gam + (1-p_i)^gam )^(1 / gam);\n    10:             w[ii] = p_i^gam / ( p_i^gam + (1-p_i)^gam )^(1 / gam);\n                       ^\n    11:     }\n  -------------------------------------------------\n\n"
     ]
    }
   ],
   "source": [
    "# Put it to true if you edit the model\n",
    "force_compilation = True\n",
    "\n",
    "# Where to save backup\n",
    "bkp_folder = 'bkp'\n",
    "os.makedirs(bkp_folder, exist_ok=True)\n",
    "bkp_file = os.path.join(bkp_folder, 'cpt_model.pkl')\n",
    "\n",
    "if not os.path.exists(bkp_file) or force_compilation is True:\n",
    "    \n",
    "    # Compile the model\n",
    "    sm = pystan.StanModel(model_code=model)\n",
    "    \n",
    "    # Save the model\n",
    "    with open(bkp_file, 'wb') as f:\n",
    "        pickle.dump(sm, f)\n",
    "else:\n",
    "    # Load the model\n",
    "    sm = pickle.load(open(bkp_file, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "* `iter`: number of samples that will be generated from each Markov chain.\n",
    "* `chains`: number of chains from which samples will be combined to form the posterior distribution. Because the underlying Markov process is stochastic, it's advantageous to have more chains that will initialise at different locations in parameter space, though adding more chains will increase the amount of time it takes to sample. \n",
    "* `warmup` (also known as 'burn-in'): number of samples that will be discarded from the beginning of each chain. As the early samples will be drawn when the Markov chain hasn't had a chance to reach equilibrium. By default this is half of iter, so for each chain we'll get 1000 samples, and chuck away the first 500. With 4 chains, we'll have 2000 samples in total.\n",
    "* `thin`: interval in sampling at which samples are retained. E.g.: if thin is equal to 3, every third sample is retained and the rest are discarded. This can be necessary to mitigate the effect of correlation between successive samples. It's set to 1 here, and so every sample is retained. \n",
    "* `seed`: Seed for the random generator. It allows for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model and generate samples\n",
    "fit = sm.sampling(data=data, iter=1000, chains=4, warmup=500, thin=1, seed=101)\n",
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the fit output to a pandas DataFrame\n",
    "summary_dict = fit.summary()\n",
    "df = pd.DataFrame(summary_dict['summary'], \n",
    "                  columns=summary_dict['summary_colnames'], \n",
    "                  index=summary_dict['summary_rownames'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of samples to plot\n",
    "n = 1000\n",
    "idx = np.random.choice(range(len(alpha)), size=n, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def f(x, alpha):\n",
    "    return x**(1-alpha)\n",
    "\n",
    "# Extract mean\n",
    "alpha_mean = df['mean']['alpha']\n",
    "\n",
    "# Extrac trace\n",
    "alpha = fit['alpha']\n",
    "\n",
    "# Create fig\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Set limits\n",
    "x_min, x_max = 0, 1\n",
    "y_min, y_max = 0, 1\n",
    "\n",
    "# Generate x-values\n",
    "x = np.linspace(x_min, x_max, 100)\n",
    "\n",
    "# Plot a subset\n",
    "for i in idx:\n",
    "    ax.plot(x, f(x, alpha[i]), color='lightsteelblue', alpha=0.005 )\n",
    "\n",
    "# Plot mean\n",
    "ax.plot(x, f(x, alpha_mean), label=\"fit\")\n",
    "\n",
    "# Plot truth\n",
    "ax.plot(x, f(x, alpha_true), ls=':', label='true', color=\"red\")\n",
    "\n",
    "# Plot bounds\n",
    "ax.plot(x, f(x, -1), color='0.01', ls='--', lw=0.5, label=\"bounds\")\n",
    "ax.plot(x, f(x, 0.5), color='0.01', ls='--', lw=0.5)\n",
    "\n",
    "# Pimp your plot\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('u(x)')\n",
    "ax.set_title('Fitted Utility function')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function given the difference of value between 2 options\n",
    "def f(x, tau):\n",
    "    return expit(x/tau)\n",
    "\n",
    "# Extract mean\n",
    "beta_mean = df['mean']['tau']\n",
    "\n",
    "# Extrac trace\n",
    "beta = fit['tau']\n",
    "\n",
    "# Create fig\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Set limits\n",
    "x_min, x_max = -1, 1\n",
    "y_min, y_max = 0, 1\n",
    "\n",
    "# Generate x-values\n",
    "x = np.linspace(x_min, x_max, 100)\n",
    "\n",
    "# Plot a subset\n",
    "for i in idx:\n",
    "    ax.plot(x, f(x, tau[i]), color='lightsteelblue', alpha=0.005)\n",
    "\n",
    "# Plot mean\n",
    "ax.plot(x, f(x, tau_mean), label=\"fit\")\n",
    "\n",
    "# Plot truth\n",
    "ax.plot(x, f(x, tau_true), ls=':', label='true', color=\"red\")\n",
    "\n",
    "# Pimp your plot\n",
    "ax.set_xlabel('$EU(L_1) - EU(L_2)$')\n",
    "ax.set_ylabel('$P(L_1)$')\n",
    "ax.set_title('Fitted Softmax function')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, beta):\n",
    "    return np.exp(-(-np.log(x) ** beta)\n",
    "\n",
    "# Create fig\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Set limits\n",
    "x_min, x_max = 0, 1\n",
    "y_min, y_max = 0, 1\n",
    "\n",
    "# Generate x-values\n",
    "x = np.linspace(x_min, x_max, 100)\n",
    "\n",
    "# Plot a subset\n",
    "for i in idx:\n",
    "    ax.plot(x, f(x, beta[i]), color='lightsteelblue', alpha=0.005)\n",
    "\n",
    "# Plot mean\n",
    "ax.plot(x, f(x, beta_mean), label=\"fit\")\n",
    "\n",
    "# Plot truth\n",
    "ax.plot(x, f(x, beta_true), ls=':', label='true', color=\"red\")\n",
    "\n",
    "# Pimp your plot\n",
    "ax.set_xlabel('$p$')\n",
    "ax.set_ylabel('$w(p)$')\n",
    "ax.set_title('Fitted subjective probability perception function')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name in (\"alpha\", \"tau\", \"beta\"):\n",
    "\n",
    "    # Extract trace\n",
    "    param = fit[param_name]\n",
    "    \n",
    "    # Summary statistics\n",
    "    mean = np.mean(param)\n",
    "    median = np.median(param)\n",
    "    cred_min, cred_max = np.percentile(param, 2.5), np.percentile(param, 97.5)\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "    \n",
    "    ax = axes[0]\n",
    "    \n",
    "    ax.plot(param)\n",
    "    ax.set_xlabel('samples')\n",
    "    ax.set_ylabel(param_name)\n",
    "    ax.axhline(mean, color='r', lw=2, linestyle='--')\n",
    "    ax.axhline(median, color='c', lw=2, linestyle='--')\n",
    "    ax.axhline(cred_min, linestyle=':', color='k', alpha=0.2)\n",
    "    ax.axhline(cred_max, linestyle=':', color='k', alpha=0.2)\n",
    "    ax.set_title('Trace and Posterior Distribution for {}'.format(param_name))\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.hist(param, 30, density=True); sns.kdeplot(param, shade=True)\n",
    "    ax.set_xlabel(param_name)\n",
    "    ax.set_ylabel('density')\n",
    "    ax.axvline(mean, color='r', lw=2, linestyle='--',label='mean')\n",
    "    ax.axvline(median, color='c', lw=2, linestyle='--',label='median')\n",
    "    ax.axvline(cred_min, linestyle=':', color='k', alpha=0.2, label='95% CI')\n",
    "    ax.axvline(cred_max, linestyle=':', color='k', alpha=0.2)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = np.finfo(float).eps\n",
    "\n",
    "def objective(param, data):\n",
    "    \n",
    "    alpha, tau, beta = param\n",
    "\n",
    "    n = len(data)\n",
    "\n",
    "    sp0 = np.exp(-(-np.log(data.p0.values)) ** beta)\n",
    "    sp1 = np.exp(-(-np.log(data.p1.values)) ** beta)\n",
    "    \n",
    "    su0 = data.x0.values ** (1 - alpha)\n",
    "    su1 = data.x1.values ** (1 - alpha)\n",
    "    \n",
    "    v0 = sp0 * su0\n",
    "    v1 = sp1 * su1\n",
    "    \n",
    "    delta = v0 - v1\n",
    "    \n",
    "    p = np.zeros((2, n))\n",
    "    p[0] = expit(delta/tau)\n",
    "    p[1] = 1 - p[0]\n",
    "\n",
    "    lls = np.log(p[0, data.c.values==0] + EPS).sum()\n",
    "    lls += np.log(p[1, data.c.values==1] + EPS).sum()\n",
    "    # Since we will look for the minimum, let's return -LLS instead of LLS\n",
    "    return -lls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounds and an initial guess\n",
    "bounds = (-1, 0.5), (0.0001, None), (0, None) \n",
    "init_guess = (0, 1)\n",
    "\n",
    "# Run the optimizer\n",
    "res = scipy.optimize.minimize(\n",
    "    fun=objective,\n",
    "    x0=init_guess,\n",
    "    bounds=bounds,\n",
    "    args=(data, ))\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A '[OptimizeResult](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult)' is returned. It contains:\n",
    "* `fun` (NumPy array): Value of objective function.\n",
    "* `hess_inv` (object): Inverse of the objective functionâ€™s Hessian; may be an approximation. Not available for all solvers. The type of this attribute may be either np.ndarray or scipy.sparse.linalg.LinearOperator. Here, it is a scipy.sparse.linalg.LinearOperator.\n",
    "* `jac` (NumPy array): Value of the Jacobian.\n",
    "* `nfev` (int): Number of evaluations of the objective functions.\n",
    "* `message` (str): Description of the cause of the termination.\n",
    "* `nit` (int): Number of iterations performed by the optimizer.\n",
    "* `njev` (int): Number of evaluations of the objective functions and of its Jacobian.\n",
    "* `status` (int): Termination status of the optimizer. Its value depends on the underlying solver. Refer to message for details.\n",
    "* `success` (bool): Whether or not the optimizer exited successfully.\n",
    "* `x` (NumPy array): the solution of the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best param and best value \n",
    "best_param = res.x\n",
    "best_value = res.fun\n",
    "\n",
    "print(\"Estimation parameters: \", best_param)\n",
    "print(\"LLS: \", - best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_est, tau_est, beta_est = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function\n",
    "def f(x, alpha):\n",
    "    return x**(1-alpha)\n",
    "\n",
    "# Create fig\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Set limits\n",
    "x_min, x_max = 0, 1\n",
    "y_min, y_max = 0, 1\n",
    "\n",
    "# Generate x-values\n",
    "x = np.linspace(x_min, x_max, 100)\n",
    "\n",
    "# Plot estimate\n",
    "ax.plot(x, f(x, alpha_est), label=\"fit\")\n",
    "\n",
    "# Plot truth\n",
    "ax.plot(x, f(x, alpha_true), ls=':', label='true', color=\"red\")\n",
    "\n",
    "# Plot bounds\n",
    "ax.plot(x, f(x, -1), color='0.01', ls='--', lw=0.5, label=\"bounds\")\n",
    "ax.plot(x, f(x, 0.5), color='0.01', ls='--', lw=0.5)\n",
    "\n",
    "# Pimp your plot\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('u(x)')\n",
    "ax.set_title('Fitted Utility function')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function given the difference of value between 2 options\n",
    "def f(x, tau):\n",
    "    return expit(x/tau)\n",
    "\n",
    "# Create fig\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Set limits\n",
    "x_min, x_max = -1, 1\n",
    "y_min, y_max = 0, 1\n",
    "\n",
    "# Generate x-values\n",
    "x = np.linspace(x_min, x_max, 100)\n",
    "\n",
    "# Plot estimate\n",
    "ax.plot(x, f(x, tau_est), label=\"fit\")\n",
    "\n",
    "# Plot truth\n",
    "ax.plot(x, f(x, tau_true), ls=':', label='true', color=\"red\")\n",
    "\n",
    "# Pimp your plot\n",
    "ax.set_xlabel('$EU(L_1) - EU(L_2)$')\n",
    "ax.set_ylabel('$P(L_1)$')\n",
    "ax.set_title('Fitted Softmax function')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability perception function\n",
    "def f(x, beta):\n",
    "    return np.exp(-(-np.log(x) ** beta)\n",
    "\n",
    "# Create fig\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Set limits\n",
    "x_min, x_max = 0, 1\n",
    "y_min, y_max = 0, 1\n",
    "\n",
    "# Generate x-values\n",
    "x = np.linspace(x_min, x_max, 100)\n",
    "\n",
    "# Plot mean\n",
    "ax.plot(x, f(x, beta_est), label=\"fit\")\n",
    "\n",
    "# Plot truth\n",
    "y_true = f(x, beta_true)\n",
    "ax.plot(x, y_true, ls=':', label='true', color=\"red\")\n",
    "\n",
    "# Pimp your plot\n",
    "ax.set_xlabel('$p$')\n",
    "ax.set_ylabel('$w(p)$')\n",
    "ax.set_title('Fitted subjective probability perception function')\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_ylim(y_min, y_max)\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
